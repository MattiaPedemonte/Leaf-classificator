{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importLibrary import *\n",
    "import ClassificationModel as CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "seed = 14\n",
    "#\"C:/Users/testb/Desktop/jupyter/grapevine_leaf/Grapevine_Leaves_Image_Dataset\"\n",
    "all_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"./Grapevine_Leaves_Image_Dataset\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',  # etichette one-hot\n",
    "    image_size=IMG_SIZE,  # ad esempio (224, 224)\n",
    "    batch_size=None  # nessun batch, ottieni tutte le immagini una per una\n",
    ")\n",
    "\n",
    "all_data = all_data.map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "# Estrai le immagini e le etichette dal dataset\n",
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "for image, label in all_data:\n",
    "    image_list.append(image.numpy())  # Converti l'immagine in NumPy array\n",
    "    label_list.append(label.numpy())  # Converti l'etichetta in NumPy array\n",
    "\n",
    "# Converti le liste in array NumPy\n",
    "image_array = np.array(image_list)\n",
    "label_array = np.array(label_list)\n",
    "\n",
    "print(f\"image_array.shape: {image_array.shape}\")\n",
    "print(f\"label_array.shape: {label_array.shape}\")\n",
    "\n",
    "# Primo split: ottieni training (70%) e temp (30%) per test+validation\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    image_array, label_array, test_size=0.3, stratify=label_array, random_state=seed  # 30% per validation+test\n",
    ")\n",
    "\n",
    "# Secondo split: dividi temp in validation (10%) e test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.66, stratify=y_temp, random_state=seed  # 2/3 per test, 1/3 per validation\n",
    ")\n",
    "\n",
    "# Convertire i dati divisi in tf.data.Dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))#.batch(BATCH_SIZE)\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))#.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.cardinality().numpy()) #num BATCH\n",
    "for image_batch, labels_batch in train_data:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch) # contiene one-hot delle classi\n",
    "    break\n",
    "\n",
    "img = image_batch[:,:,:].numpy() #.astype(np.uint8)\n",
    "# Converti l'immagine da BGR a RGB (OpenCV usa BGR per default)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Crea una figura e un'asse\n",
    "plt.figure(figsize=(8, 8))  # Puoi regolare le dimensioni della figura\n",
    "plt.imshow(img)  # Mostra l'immagine\n",
    "plt.axis('off')  # Disabilita gli assi\n",
    "plt.show()  # Mostra la figura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for data augmentation\n",
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)  # Randomly flip the image\n",
    "    # image = tf.image.random_brightness(image, max_delta=0.1)  # Randomly change brightness\n",
    "    # image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Randomly adjust contrast\n",
    "    return image, label\n",
    "\n",
    "def random_rotate_image(image, label):\n",
    "    # Scegliamo casualmente tra 1 (90 gradi), 2 (180 gradi) e 3 (270 gradi)\n",
    "    k = np.random.choice([1, 2, 3])\n",
    "    return tf.image.rot90(image, k=k), label\n",
    "\n",
    "# Apply the augmentation function\n",
    "augmented_data = train_data.map(random_rotate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte il dataset in un iteratore numpy\n",
    "augmented_image, augmented_label = [], []\n",
    "for images, labels in train_data.as_numpy_iterator():\n",
    "    augmented_image.append(images)\n",
    "    augmented_label.append(labels)\n",
    "    \n",
    "print(len(augmented_image))\n",
    "print(augmented_image[0].shape)\n",
    "\n",
    "img = augmented_image[0]#.astype(np.uint8)\n",
    "# Converti l'immagine da BGR a RGB (OpenCV usa BGR per default)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Crea una figura e un'asse\n",
    "plt.figure(figsize=(8, 8))  # Puoi regolare le dimensioni della figura\n",
    "plt.imshow(img)  # Mostra l'immagine\n",
    "plt.axis('off')  # Disabilita gli assi\n",
    "plt.show()  # Mostra la figura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data = train_data\n",
    "\n",
    "## scommentare se voglio unire anche augmented data al training set ##\n",
    "# Combiniamo i due dataset (originale e aumentato)\n",
    "combined_data = train_data.concatenate(augmented_data)\n",
    "\n",
    "# Optional: Shuffling e batching del nuovo dataset esteso\n",
    "combined_data = combined_data.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "size =0\n",
    "num_class_elem = np.zeros((1,5))\n",
    "IMG = []\n",
    "LABEL_IMG = []\n",
    "for img,l in combined_data:\n",
    "    size += img.shape[0]\n",
    "    num_class_elem += np.sum(l, axis=0) \n",
    "\n",
    "    for index, label in enumerate(l):\n",
    "        idx = np.argmax(label)\n",
    "        if idx not in LABEL_IMG:\n",
    "            IMG.append(img[index])\n",
    "            LABEL_IMG.append(idx)\n",
    "        if len(LABEL_IMG)==5:\n",
    "            break\n",
    "    \n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for i in range(5):\n",
    "    ax = axs[i // 3, i % 3]\n",
    "    ax.imshow(IMG[i])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(LABEL_IMG[i])\n",
    "ax = axs[1,2]\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(size)\n",
    "print(num_class_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing images\n",
    "path = \"./Grapevine_Leaves_Image_Dataset/Nazli\"\n",
    "# Get a list of all image file names in the directory\n",
    "image_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "# Display the first 6 images with their labels\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for i in range(6):\n",
    "    # Get the image file name and its label\n",
    "    image_file = image_files[i]\n",
    "    label = image_file.split('.')[0]\n",
    "\n",
    "    # Load and display the image\n",
    "    img_path = os.path.join(path, image_file)\n",
    "    img = mpimg.imread(img_path)\n",
    "    ax = axs[i // 3, i % 3]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]\n",
    "#n= \"resNet_fineTuned_\"\n",
    "n = \"VGG19_finetuned_\"\n",
    "if n==\"resNet_fineTuned_\":\n",
    "    model = CM.GenerateResModel(num_classes)\n",
    "elif n==\"CNN_\":\n",
    "    model = CM.GenerateCNN(num_classes)\n",
    "elif n==\"VGG16_finetuned_\":\n",
    "    model = CM.GenerateVGG16Model(num_classes)  \n",
    "elif n==\"VGG19_finetuned_\":\n",
    "    model = CM.GenerateVGG19Model(num_classes)  \n",
    "else:\n",
    "    print(\"not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Imposta il callback di Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Metric da monitorare (ad esempio la perdita sul validation set)\n",
    "    patience=5,                # Numero di epoche di pazienza (ad esempio, 5)\n",
    "    restore_best_weights=True  # Ripristina i pesi con le migliori prestazioni sul validation set\n",
    ")\n",
    "\n",
    "def TrainModel(model: Model, alpha: float = 0.0001, epochs: int = 20):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=alpha), metrics=['categorical_accuracy'])\n",
    "    #train_d = train_data.batch(BATCH_SIZE)\n",
    "    \n",
    "    history = model.fit(\n",
    "        combined_data,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=validation_data,  # Batch di validazione separato con dimensione diversa\n",
    "        #callbacks=[early_stopping]\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the single long training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = TrainModel(model, alpha=0.0001, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to split the training in shorter training and save the models - aspecially without GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = []\n",
    "# for i in range(1,3):\n",
    "#     print(\"Training ... N° \" + str(i))\n",
    "#     if (i!=1):\n",
    "#         model = tf.keras.models.load_model('./'+n+str((i-1)*20)+'.keras')\n",
    "#     history = TrainModel(model, alpha=0.0001, epochs=20)\n",
    "#     H.append(history)\n",
    "#     name = n+str(i*20)\n",
    "#     model.save('./'+name+'.keras')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model with \"idx_model\" number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_model = 20\n",
    "# model = tf.keras.models.load_model('./'+n+str(idx_model)+'.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotHistory(history) -> None:\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['categorical_accuracy'], label='train accuracy') #categorical_accuracy\n",
    "    # plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    # plt.plot(history.history['val_loss'], label='val loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotHistory(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "def PlotMetrics(max_indices_val, max_indices_pred):\n",
    "    # Create the confusion matrix\n",
    "    nn_cm = confusion_matrix(max_indices_val, max_indices_pred)\n",
    "    accuracy = accuracy_score(max_indices_val, max_indices_pred)\n",
    "    precision = precision_score(max_indices_val, max_indices_pred, average='macro')\n",
    "    recall = recall_score(max_indices_val, max_indices_pred, average='macro')\n",
    "    f1 = f1_score(max_indices_val, max_indices_pred, average='macro')\n",
    "\n",
    "    print(nn_cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "# print(y_pred)\n",
    "# print(y_test)\n",
    "max_indices_pred = np.argmax(y_pred, axis=1) #prediction as class number\n",
    "max_indices_val = np.argmax(y_train, axis=1) #prediction as class number\n",
    "PlotMetrics(max_indices_val, max_indices_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "label_list = []\n",
    "for image, label in combined_data:\n",
    "    image_list.append(image.numpy())  # Converti l'immagine in NumPy array\n",
    "    label_list.append(label.numpy())  # Converti l'etichetta in NumPy array\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_comb = np.concatenate(image_list, axis=0)\n",
    "y_comb = np.concatenate(label_list, axis=0)\n",
    "\n",
    "print(X_comb.shape)\n",
    "\n",
    "y_pred = model.predict(X_comb)\n",
    "\n",
    "# print(y_pred[0:10])\n",
    "max_indices_pred = np.argmax(y_pred, axis=1) #prediction as class number\n",
    "# print(max_indices_pred[0:10])\n",
    "max_indices_val = np.argmax(y_comb, axis=1) #prediction as class number\n",
    "# print(max_indices_val[0:10])\n",
    "\n",
    "PlotMetrics(max_indices_val, max_indices_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred)\n",
    "# print(y_test)\n",
    "max_indices_pred = np.argmax(y_pred, axis=1) #prediction as class number\n",
    "max_indices_val = np.argmax(y_test, axis=1) #prediction as class number\n",
    "\n",
    "\n",
    "PlotMetrics(max_indices_val, max_indices_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('./my_model.keras')\n",
    "h = TrainModel(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotHistory(history)\n",
    "y_pred = model.predict(X_val)\n",
    "print(y_pred)\n",
    "print(y_val)\n",
    "max_indices_pred = np.argmax(y_pred, axis=1) #prediction as class number\n",
    "max_indices_val = np.argmax(y_val, axis=1) #prediction as class number\n",
    "PlotMetrics(max_indices_pred,max_indices_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
